{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b895920",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Time Series\n",
    "\n",
    "*Note*: There have been relatively large (and exciting!) updates to `pymc3` since the Bayesian statistics lectures. This includes the PyMC3 team officially taking over the Theano project and renaming it `aesara` -- Any of the functionality that we had previously used should be effectively the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a8e92",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# aesara is missing a couple pieces of functionality\n",
    "# still so we still use theano...\n",
    "# import aesara\n",
    "# import aesara.tensor as at\n",
    "import theano as aesara\n",
    "import theano.tensor as at\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numba\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1815f5f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Refresher\n",
    "\n",
    "Let's start spend a few minutes refreshing our knowledge of Bayesian methods.\n",
    "\n",
    "We use the following notation:\n",
    "\n",
    "* $y$ represents data\n",
    "* $\\theta$ represents parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf16887",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Bayes Law**:\n",
    "\n",
    "$$P(\\theta | y) = \\frac{P(\\theta) P(y | \\theta)}{P(\\theta)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34beea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Prior**: $P(\\theta)$\n",
    "\n",
    "The prior represents your individual beliefs about the probability of certain combinations of parameters having generated the data of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3b350",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Likelihood**: $P(y | \\theta)$\n",
    "\n",
    "The likelihood function is a function that maps values of the parameters, $\\theta \\in \\Theta$ and observations, $y \\in \\mathcal{Y}$, into the probability of having observed $y$ given $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db429df0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Posterior**: $P(\\theta | y)$\n",
    "\n",
    "The posterior combines the prior and likelihood to specify what your beliefs about what the probability of certain combinations parameters having generated the data (after observing data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a419c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Markov Chain Monte-Carlo (MCMC)**:\n",
    "\n",
    "A class of methods used to simulate samples from the posterior distribution.\n",
    "\n",
    "We have previously discussed the Metropolis-Hastings algorithm, but, it is dated, and, as we mentioned, modern software (like `pymc3`) typically samples using the no u-turn sampler.\n",
    "\n",
    "If you want a graphical refresher for how these algorithms work, we refer you back to this (excellent) [online tool](https://chi-feng.github.io/mcmc-demo/app.html) developed by [Chi Feng](https://github.com/chi-feng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4fca7d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Why do I like Bayesian models?**\n",
    "\n",
    "* Partially driven by beliefs of people that I respect and learned from\n",
    "* Partially that the Bayesian notion of probability (beliefs about the likelihood of something occurring rather than frequencies with which something occurs) makes more sense to me\n",
    "* Significantly motivated by their ability to quantify uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be05489",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced PyMC3 Usage: Aesara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c48fab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**What is Aesara?**\n",
    "\n",
    ">Aesara is not a programming language in the normal sense because you write a program in Python that builds expressions for Aesara. Still it is like a programming language in the sense that you have to\n",
    ">\n",
    ">* declare variables (a, b) and give their types\n",
    ">* build expressions for how to put those variables together\n",
    ">* compile expression graphs to functions in order to use them for computation.\n",
    ">\n",
    "> It is good to think of `aesara.function` as the interface to a compiler which builds a callable object from a purely symbolic graph. One of Aesara’s most important features is that `aesara.function` can optimize a graph and even compile some or all of it into native machine instructions.\n",
    ">\n",
    ">\\- _From [Aesara documentation](https://aesara.readthedocs.io/en/latest/index.html)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ee512",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    ">Aesara’s compiler applies many optimizations of varying complexity to these symbolic expressions. These optimizations include, but are not limited to:\n",
    ">\n",
    ">* use of GPU for computations\n",
    ">* constant folding\n",
    ">* merging of similar subgraphs, to avoid redundant calculation\n",
    ">* arithmetic simplification (e.g. x*y/x -> y, --x -> x)\n",
    ">* inserting efficient BLAS operations (e.g. GEMM) in a variety of contexts\n",
    ">* using memory aliasing to avoid calculation\n",
    ">* using inplace operations wherever it does not interfere with aliasing\n",
    ">* loop fusion for elementwise sub-expressions\n",
    ">* improvements to numerical stability (e.g. \\log(1+\\exp(x)) and \\log(\\sum_i \\exp(x[i])))\n",
    ">* and [many more](https://aesara.readthedocs.io/en/latest/optimizations.html#optimizations)\n",
    ">\n",
    ">\\- _From [Aesara documentation](https://aesara.readthedocs.io/en/latest/index.html)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56f7ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Fun fact: The library that Aesara is based on, Theano, was written at the LISA lab to support rapid development of efficient machine learning algorithms. Theano was named after the Greek mathematician, who may have been Pythagoras’ wife. Aesara is an alleged daughter of Pythagoras and Theano.\n",
    ">\n",
    ">\\- _From [Aesara documentation](https://aesara.readthedocs.io/en/latest/index.html)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc473e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example program**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268c298",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# declare two symbolic floating-point scalars\n",
    "a = at.dscalar()\n",
    "b = at.dscalar()\n",
    "\n",
    "# create a simple expression\n",
    "c = a + b\n",
    "\n",
    "# convert the expression into a callable object that takes `(a, b)`\n",
    "# values as input and computes a value for `c`\n",
    "f = aesara.function([a, b], c)\n",
    "\n",
    "# bind 1.5 to 'a', 2.5 to 'b', and evaluate 'c'\n",
    "assert 4.0 == f(1.5, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee8419",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Scan**\n",
    "\n",
    "One of the reasons that we're talking about Aesara today is because many time series models require us to write loops within our PyMC3 models. This is slow because the (potential) type instability associated with these loops prevents Aesara from being able to make the optimizations that make PyMC3 so fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aab8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "k = at.iscalar(\"k\")\n",
    "A = at.vector(\"A\")\n",
    "\n",
    "# Symbolic description of the result\n",
    "result, updates = aesara.scan(\n",
    "    fn=lambda prior_result, A: prior_result * A,\n",
    "    outputs_info=at.ones_like(A),\n",
    "    non_sequences=A, n_steps=k\n",
    ")\n",
    "\n",
    "# We only care about A**k, but scan has provided us with A**1 through A**k.\n",
    "# Discard the values that we don't care about. Scan is smart enough to\n",
    "# notice this and not waste memory saving them.\n",
    "final_result = result[-1]\n",
    "\n",
    "# compiled function that returns A**k\n",
    "power = aesara.function(inputs=[A, k], outputs=final_result, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08008738",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(power(range(10), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e68f79d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(power(range(10), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836bddf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We will see more examples soon -- The important thing is understanding that `scan` performs the \"compilation\" step and the theoretical outputs are stored in `result`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09e1a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## \"Warm-up\": Revisiting AR/MA models\n",
    "\n",
    "We begin by revisiting the models we discussed previously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96fc3f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Autoregressive**\n",
    "\n",
    "\\begin{align*}\n",
    "  y_{t+1} &= \\rho_1 y_t + \\rho_2 y_{t-1} + \\sigma \\varepsilon_{t+1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abf1b6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def simulate_ar2(T, rho_1, rho_2, sigma):\n",
    "    # Allocate space\n",
    "    out = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        if t < 2:\n",
    "            out[t] = 0\n",
    "        else:\n",
    "            out[t] = rho_1*out[t-1] + rho_2*out[t-2] + sigma*np.random.randn()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811efd6c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ar_data = simulate_ar2(250, 0.7, -0.2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a83e4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ab999b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def one_step(eps, ym1, ym2, rho_1, rho_2, sigma):\n",
    "    return rho_1*ym1 + rho_2*ym2 + sigma*eps\n",
    "\n",
    "y = at.vector(\"y\")\n",
    "eps = at.vector(\"eps\")\n",
    "rho_1 = at.dscalar()\n",
    "rho_2 = at.dscalar()\n",
    "sigma = at.dscalar()\n",
    "\n",
    "results, updates = aesara.scan(\n",
    "    one_step, sequences=[\n",
    "        dict(input=eps, taps=[0])\n",
    "    ], outputs_info=dict(\n",
    "        initial=y, taps=[-1, -2]\n",
    "    ),\n",
    "    non_sequences=[rho_1, rho_2, sigma]\n",
    ")\n",
    "\n",
    "ar_stepper = aesara.function(\n",
    "    inputs=[eps, y, rho_1, rho_2, sigma],\n",
    "    outputs=results, updates=updates\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4addd691",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ar_scan = ar_stepper(np.random.randn(100), np.zeros(2), 0.9, -0.2, 0.1)\n",
    "\n",
    "plt.plot(ar_scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649df22",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Computing the model by hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e9f7b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nobs = ar_data.shape[0]\n",
    "ar_fl = pm.Model()\n",
    "\n",
    "with ar_fl:\n",
    "    # Data\n",
    "    data_fl = pm.Data(\"data\", ar_data)\n",
    "\n",
    "    # Parameters\n",
    "    rho_1 = pm.Uniform(\"rho_1\", -1, 1)\n",
    "    rho_2 = pm.Uniform(\"rho_2\", -1, 1)\n",
    "    sigma = pm.HalfNormal(\"sigma\", 3)\n",
    "\n",
    "    # Update via loop\n",
    "    for t in range(2, nobs):\n",
    "        obs = pm.Normal(\n",
    "            f\"obs_{t}\", rho_1*data_fl[t-1] + rho_2*data_fl[t-2],\n",
    "            sigma, observed=data_fl[t]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11425c48",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Not going to run this live -- It takes a few minutes to compile\n",
    "# the model and initialize the sampler... Gets slower as number\n",
    "# of observations increase...\n",
    "\n",
    "# with ar_fl:\n",
    "#     trace_fl = pm.sample(750, return_inferencedata=False)\n",
    "#     az.plot_trace(trace_fl, var_names=[\"rho_1\", \"rho_2\", \"sigma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f464371",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Computed via scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df47e6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ar_bh = pm.Model()\n",
    "\n",
    "with ar_bh:\n",
    "    # Data\n",
    "    data_bh = pm.Data(\"data\", ar_data)\n",
    "\n",
    "    # Parameters\n",
    "    rho_1 = pm.Uniform(\"rho_1\", -1, 1)\n",
    "    rho_2 = pm.Uniform(\"rho_2\", -1, 1)\n",
    "    sigma = pm.HalfNormal(\"sigma\", 3)\n",
    "\n",
    "    # Update via scan\n",
    "    def one_step_mu(ytm1, ytm2, rho_1, rho_2):\n",
    "        return rho_1*ytm1 + rho_2*ytm2\n",
    "\n",
    "    _mu, update = aesara.scan(\n",
    "        one_step_mu,\n",
    "        sequences=[dict(input=data_bh, taps=[-1, -2])],\n",
    "        non_sequences=[rho_1, rho_2]\n",
    "    )\n",
    "\n",
    "    obs = pm.Normal(\"obs\", _mu, sigma, observed=data_bh[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262df84",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ar_bh:\n",
    "    trace_bh = pm.sample(750, return_inferencedata=False)\n",
    "    az.plot_trace(trace_bh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8725607b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e53adf0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ar_vec = pm.Model()\n",
    "\n",
    "with ar_vec:\n",
    "    # Data\n",
    "    data_vec = pm.Data(\"data\", ar_data)\n",
    "\n",
    "    # Parameters\n",
    "    rho_1 = pm.Uniform(\"rho_1\", -1, 1)\n",
    "    rho_2 = pm.Uniform(\"rho_2\", -1, 1)\n",
    "    sigma = pm.HalfNormal(\"sigma\", 3)\n",
    "\n",
    "    # Compute mean\n",
    "    mu = rho_1*data_bh[1:-1] + rho_2*data_bh[0:-2]\n",
    "    obs = pm.Normal(\"obs\", mu, sigma, observed=data_vec[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692584ed",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ar_vec:\n",
    "    trace_vec = pm.sample(750, return_inferencedata=False)\n",
    "    az.plot_trace(trace_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e003ffa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using PyMC3 functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762957e4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ar_pymc3 = pm.Model()\n",
    "\n",
    "with ar_pymc3:\n",
    "    # Data\n",
    "    data_pymc3 = pm.Data(\"data\", ar_data)\n",
    "\n",
    "    # Parameters\n",
    "    rho_1 = pm.Uniform(\"rho_1\", -1, 1)\n",
    "    rho_2 = pm.Uniform(\"rho_2\", -1, 1)\n",
    "    sigma = pm.HalfNormal(\"sigma\", 3)\n",
    "\n",
    "    obs = pm.AR(\"observed\", [rho_1, rho_2], sigma, observed=data_pymc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa5c27",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with ar_pymc3:\n",
    "    trace_pymc3 = pm.sample(750, return_inferencedata=False)\n",
    "    az.plot_trace(trace_pymc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b9c94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving average models\n",
    "\n",
    "\\begin{align*}\n",
    "  y_{t+1} &= \\phi_1 \\varepsilon_{t} + \\phi_2 \\varepsilon_{t-1} + \\varepsilon_{t+1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a8d31",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def simulate_ma2(T, phi_1, phi_2, sigma):\n",
    "    # Allocate space\n",
    "    epsilons = sigma*np.random.randn(T+2)\n",
    "    out = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        out[t] = (\n",
    "            phi_1*epsilons[t+2-1] + \n",
    "            phi_2*epsilons[t+2-2] + \n",
    "            epsilons[t+2]\n",
    "        )\n",
    "\n",
    "    return out, epsilons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac15803",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ma_data, ma_epsilons = simulate_ma2(50, 0.75, 0.25, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce69060",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ma_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538a494",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we build this Bayesian model?\n",
    "\n",
    "* We need to compute the $\\{\\varepsilon_t\\}$\n",
    "* Given $y_0$, $\\varepsilon_{-1}$, and $\\varepsilon_{-2}$, we can compute $\\varepsilon_0$... Given $y_t$, $\\varepsilon_{t-1}$, and $\\varepsilon_{t-2}$, then we can compute $\\varepsilon_t$...\n",
    "* Let's place priors over $\\varepsilon_{t-1}$ and $\\varepsilon_{t-2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4653a3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ma_scan = pm.Model()\n",
    "\n",
    "with ma_scan:\n",
    "    # Data\n",
    "    data_scan = pm.Data(\"data\", ma_data)\n",
    "\n",
    "    # Parameters\n",
    "    phi_1 = pm.Normal(\"phi_1\", 0, 2.5)\n",
    "    phi_2 = pm.Normal(\"phi_2\", 0, 2.5)\n",
    "    sigma = pm.HalfNormal(\"sigma\", 5)\n",
    "\n",
    "    # Unobserved epsilons\n",
    "    eps_m2m1 = pm.Normal(\"eps_m2m1\", 0, sigma/2, shape=2)\n",
    "\n",
    "    # Update via scan\n",
    "    def compute_eps_t(yt, epsm2, epsm1, phi_2, phi_1):\n",
    "        return (yt - phi_1*epsm1 - phi_2*epsm2)\n",
    "\n",
    "    _eps, update = aesara.scan(\n",
    "        compute_eps_t,\n",
    "        sequences=[dict(input=data_scan, taps=[0])],\n",
    "        outputs_info=[\n",
    "            dict(initial=eps_m2m1, taps=[-2, -1])\n",
    "        ],\n",
    "        non_sequences=[phi_2, phi_1]\n",
    "    )\n",
    "    epsilons = pm.Deterministic(\"epsilons\", _eps)\n",
    "\n",
    "    obs0 = pm.Normal(\n",
    "        \"obs0\", phi_1*eps_m2m1[1] + phi_2*eps_m2m1[0],\n",
    "        sigma, observed=data_scan[0]\n",
    "    )\n",
    "    obs1 = pm.Normal(\n",
    "        \"obs1\", phi_1*epsilons[0] + phi_2*eps_m2m1[1],\n",
    "        sigma, observed=data_scan[1]\n",
    "    )\n",
    "    obs = pm.Normal(\n",
    "        \"obs\", phi_1*epsilons[1:-1] + phi_2*epsilons[0:-2], sigma,\n",
    "        observed=data_scan[2:]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ef1b26",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with ma_scan:\n",
    "    trace_ma = pm.sample(\n",
    "        1000, return_inferencedata=True,\n",
    "        tune=1500, target_accept=0.95\n",
    "    )\n",
    "    az.plot_trace(trace_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5952e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "posterior_epsilons = trace_ma[\"posterior\"][\"epsilons\"]\n",
    "\n",
    "posterior_epsilons_quantiles = posterior_epsilons.quantile(\n",
    "    [0.05, 0.5, 0.95], dim=[\"chain\", \"draw\"]\n",
    ").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a778cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = ma_epsilons.shape[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Realized epsilons\n",
    "ax.plot(ma_epsilons, color=\"k\", linestyle=\"-\", linewidth=1.5)\n",
    "\n",
    "# Quantiles and median\n",
    "ax.plot(\n",
    "    np.arange(2, T),\n",
    "    posterior_epsilons_quantiles[1, :],\n",
    "    color=\"ForestGreen\", linestyle=\"--\", linewidth=1.5\n",
    ")\n",
    "ax.fill_between(\n",
    "    np.arange(2, T),\n",
    "    posterior_epsilons_quantiles[0, :],\n",
    "    posterior_epsilons_quantiles[2, :],\n",
    "    color=\"ForestGreen\", alpha=0.5\n",
    ")\n",
    "\n",
    "ax.set_title(r\"Realized vs Estimated $\\varepsilon$s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e4e97",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian decomposable time series models\n",
    "\n",
    "We spoke about additive decompositions in a previous lecture. It is a method that aims to describe a time series data using the sum of certain components (trend, seasonal, etc...).\n",
    "\n",
    "One relatively recent tool that has been released that does Bayesian time series decomposition is an open source library developed by Facebook called \"Prophet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58e28d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Prophet**\n",
    "\n",
    "[Prophet](https://facebook.github.io/prophet/) is an excellent piece of software that implements many sensible defaults (especially when being applied to data similar to things that Facebook might be interested in).\n",
    "\n",
    "However, we also want to acknowledge the PyMC3-based ([`pm-prophet`](https://github.com/luke14free/pm-prophet)). The benefit to this package over the original Prophet is that they have left the flexibility to modify the priors and other components of the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aebc3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The model\n",
    "\n",
    "Prophet (and the PyMC3 based PM-Prophet) build a model of the following format:\n",
    "\n",
    "$$y_t = g(t) + s(t) + h(t) + \\varepsilon_t$$\n",
    "\n",
    "where\n",
    "\n",
    "* $y_t$ is the observation at period $t$\n",
    "* $\\varepsilon_t$ is noise\n",
    "* $g(t)$ is a trend component\n",
    "* $s(t)$ is a collection of seasonal components\n",
    "* $h(t)$ is a holiday component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6f173",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Trend component**\n",
    "\n",
    "Original Prophet model supports either:\n",
    "\n",
    "* Piecewise-linear trends with specified changepoints\n",
    "* Piecewise-logistic growth\n",
    "\n",
    "In both cases, subsets of the parameters are allowed to change as time passes on to account for time-varying trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81efdbc7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Seasonal component**\n",
    "\n",
    "Uses Fourier series to approximate generic (smooth) seasonality in the data\n",
    "\n",
    "$$s(t) = \\sum_{n=1}^N \\left(a_n \\cos\\left(\\frac{2 \\pi n t}{P}\\right) + b_n \\sin\\left(\\frac{2 \\pi n t}{P}\\right) \\right)$$\n",
    "\n",
    "Need to pick:\n",
    "\n",
    "* $P$: The period to approximate (i.e. 7 for weekly seasonality, 365.25 for yearly, etc...)\n",
    "* $N$: The number of Fourier series elements to include -- Higher $N$ allows for more flexible fitting but raises risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0730b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Holiday component**\n",
    "\n",
    "Each holiday is given a window and a magnitude parameter. Let $D_h = [h - j, h + j]$ then a day is assigned an indicator of 1 if it falls in the window $D_h$ and the magnitude of the effect is measured as $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e4e84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applications\n",
    "\n",
    "[Prophet paper](https://peerj.com/preprints/3190/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0388c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fun blogposts that are worth reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590687c5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* [Sorry ARIMA, but I’m Going Bayesian](https://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/)\n",
    "* [Tis the Season... to be Bayesian!](https://multithreaded.stitchfix.com/blog/2020/12/16/tis-the-season-to-be-bayesian/)\n",
    "* [Talk by one of main developers of Prophet](https://www.youtube.com/watch?v=pOYAXv15r3A&ab_channel=LanderAnalytics)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
