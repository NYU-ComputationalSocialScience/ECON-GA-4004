{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A First Look at the Kalman Filter\n",
    "\n",
    "\n",
    "<a id='index-0'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This lecture provides a simple and intuitive introduction to the Kalman filter\n",
    "\n",
    "Required knowledge: Familiarity with matrix manipulations, multivariate normal distributions, covariance matrices, etc.\n",
    "\n",
    "We’ll need the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import multivariate_normal\n",
    "from scipy import linalg\n",
    "from scipy.linalg import eigvals\n",
    "from scipy.integrate import quad\n",
    "from scipy.stats import norm\n",
    "from quantecon import Kalman, LinearStateSpace\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (11, 5)  #set default figure size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Basic Idea\n",
    "\n",
    "The Kalman filter has many applications in economics, but for now\n",
    "let’s pretend that we are rocket scientists.\n",
    "\n",
    "A missile has been launched from country Y and our mission is to track it.\n",
    "\n",
    "Let $ x  \\in \\mathbb{R}^2 $ denote the current location of the missile—a\n",
    "pair indicating latitude-longitude coordinates on a map.\n",
    "\n",
    "At the present moment in time, the precise location $ x $ is unknown, but\n",
    "we do have some beliefs about $ x $.\n",
    "\n",
    "One way to summarize our knowledge is a point prediction $ \\hat x $\n",
    "\n",
    "- But what if the President wants to know the probability that the missile is currently over the Sea of Japan?  \n",
    "- Then it is better to summarize our initial beliefs with a bivariate probability density $ p $  \n",
    "  - $ \\int_E p(x)dx $ indicates the probability that we attach to the missile being in region $ E $.  \n",
    "\n",
    "\n",
    "The density $ p $ is called our *prior* for the random variable $ x $.\n",
    "\n",
    "To keep things tractable in our example,  we  assume that our prior is Gaussian.\n",
    "\n",
    "In particular, we take\n",
    "\n",
    "\n",
    "<a id='equation-prior'></a>\n",
    "$$\n",
    "p = N(\\hat x, \\Sigma) \\tag{1}\n",
    "$$\n",
    "\n",
    "where $ \\hat x $ is the mean of the distribution and $ \\Sigma $ is a\n",
    "$ 2 \\times 2 $ covariance matrix.  In our simulations, we will suppose that\n",
    "\n",
    "\n",
    "<a id='equation-kalman-dhxs'></a>\n",
    "$$\n",
    "\\hat x\n",
    "= \\left(\n",
    "\\begin{array}{c}\n",
    "    0.2 \\\\\n",
    "    -0.2\n",
    "\\end{array}\n",
    "  \\right),\n",
    "\\qquad\n",
    "\\Sigma\n",
    "= \\left(\n",
    "\\begin{array}{cc}\n",
    "    0.4 & 0.3 \\\\\n",
    "    0.3 & 0.45\n",
    "\\end{array}\n",
    "  \\right) \\tag{2}\n",
    "$$\n",
    "\n",
    "This density $ p(x) $ is shown below as a contour map, with the center of the red ellipse being equal to $ \\hat x $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the Gaussian prior density p\n",
    "Σ = [[0.4, 0.3], [0.3, 0.45]]\n",
    "Σ = np.matrix(Σ)\n",
    "x_hat = np.matrix([0.2, -0.2]).T\n",
    "\n",
    "# Define the matrices G and R from the equation y = G x + N(0, R)\n",
    "G = [[1, 0], [0, 1]]\n",
    "G = np.matrix(G)\n",
    "R = 0.5 * Σ\n",
    "\n",
    "# The matrices A and Q\n",
    "A = [[1.2, 0], [0, -0.2]]\n",
    "A = np.matrix(A)\n",
    "Q = 0.3 * Σ\n",
    "\n",
    "# The observed value of y\n",
    "y = np.matrix([2.3, -1.9]).T\n",
    "\n",
    "# Set up grid for plotting\n",
    "x_grid = np.linspace(-1.5, 2.9, 100)\n",
    "y_grid = np.linspace(-3.1, 1.7, 100)\n",
    "X, Y = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "def bivariate_normal(x, y, σ_x=1.0, σ_y=1.0, μ_x=0.0, μ_y=0.0, σ_xy=0.0):\n",
    "    \"\"\"\n",
    "    Compute and return the probability density function of bivariate normal\n",
    "    distribution of normal random variables x and y\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like(float)\n",
    "        Random variable\n",
    "\n",
    "    y : array_like(float)\n",
    "        Random variable\n",
    "\n",
    "    σ_x : array_like(float)\n",
    "          Standard deviation of random variable x\n",
    "\n",
    "    σ_y : array_like(float)\n",
    "          Standard deviation of random variable y\n",
    "\n",
    "    μ_x : scalar(float)\n",
    "          Mean value of random variable x\n",
    "\n",
    "    μ_y : scalar(float)\n",
    "          Mean value of random variable y\n",
    "\n",
    "    σ_xy : array_like(float)\n",
    "           Covariance of random variables x and y\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x_μ = x - μ_x\n",
    "    y_μ = y - μ_y\n",
    "\n",
    "    ρ = σ_xy / (σ_x * σ_y)\n",
    "    z = x_μ**2 / σ_x**2 + y_μ**2 / σ_y**2 - 2 * ρ * x_μ * y_μ / (σ_x * σ_y)\n",
    "    denom = 2 * np.pi * σ_x * σ_y * np.sqrt(1 - ρ**2)\n",
    "    return np.exp(-z / (2 * (1 - ρ**2))) / denom\n",
    "\n",
    "def gen_gaussian_plot_vals(μ, C):\n",
    "    \"Z values for plotting the bivariate Gaussian N(μ, C)\"\n",
    "    m_x, m_y = float(μ[0]), float(μ[1])\n",
    "    s_x, s_y = np.sqrt(C[0, 0]), np.sqrt(C[1, 1])\n",
    "    s_xy = C[0, 1]\n",
    "    return bivariate_normal(X, Y, s_x, s_y, m_x, m_y, s_xy)\n",
    "\n",
    "# Plot the figure\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.grid()\n",
    "\n",
    "Z = gen_gaussian_plot_vals(x_hat, Σ)\n",
    "ax.contourf(X, Y, Z, 6, alpha=0.6, cmap=cm.jet)\n",
    "cs = ax.contour(X, Y, Z, 6, colors=\"black\")\n",
    "ax.clabel(cs, inline=1, fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Filtering Step\n",
    "\n",
    "We are now presented with some good news and some bad news.\n",
    "\n",
    "The good news is that the missile has been located by our sensors, which report that the current location is $ y = (2.3, -1.9) $.\n",
    "\n",
    "The next figure shows the original prior $ p(x) $ and the new reported\n",
    "location $ y $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.grid()\n",
    "\n",
    "Z = gen_gaussian_plot_vals(x_hat, Σ)\n",
    "ax.contourf(X, Y, Z, 6, alpha=0.6, cmap=cm.jet)\n",
    "cs = ax.contour(X, Y, Z, 6, colors=\"black\")\n",
    "ax.clabel(cs, inline=1, fontsize=10)\n",
    "ax.text(float(y[0]), float(y[1]), \"$y$\", fontsize=20, color=\"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The bad news is that our sensors are imprecise.\n",
    "\n",
    "In particular, we should interpret the output of our sensor not as\n",
    "$ y=x $, but rather as\n",
    "\n",
    "\n",
    "<a id='equation-kl-measurement-model'></a>\n",
    "$$\n",
    "y = G x + v, \\quad \\text{where} \\quad v \\sim N(0, R) \\tag{3}\n",
    "$$\n",
    "\n",
    "Here $ G $ and $ R $ are $ 2 \\times 2 $ matrices with $ R $\n",
    "positive definite.  Both are assumed known, and the noise term $ v $ is assumed\n",
    "to be independent of $ x $.\n",
    "\n",
    "How then should we combine our prior $ p(x) = N(\\hat x, \\Sigma) $ and this\n",
    "new information $ y $ to improve our understanding of the location of the\n",
    "missile?\n",
    "\n",
    "As you may have guessed, the answer is to use Bayes’ theorem, which tells\n",
    "us to  update our prior $ p(x) $ to $ p(x \\,|\\, y) $ via\n",
    "\n",
    "$$\n",
    "p(x \\,|\\, y) = \\frac{p(y \\,|\\, x) \\, p(x)} {p(y)}\n",
    "$$\n",
    "\n",
    "where $ p(y) = \\int p(y \\,|\\, x) \\, p(x) dx $.\n",
    "\n",
    "In solving for $ p(x \\,|\\, y) $, we observe that\n",
    "\n",
    "- $ p(x) = N(\\hat x, \\Sigma) $.  \n",
    "- In view of [(18.3)](#equation-kl-measurement-model), the conditional density $ p(y \\,|\\, x) $ is $ N(Gx, R) $.  \n",
    "- $ p(y) $ does not depend on $ x $, and enters into the calculations only as a normalizing constant.  \n",
    "\n",
    "\n",
    "Because we are in a linear and Gaussian framework, the updated density can be computed by calculating population linear regressions.\n",
    "\n",
    "In particular, the solution is known <sup><a href=#f1 id=f1-link>[1]</a></sup> to be\n",
    "\n",
    "$$\n",
    "p(x \\,|\\, y) = N(\\hat x^F, \\Sigma^F)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "<a id='equation-kl-filter-exp'></a>\n",
    "$$\n",
    "\\hat x^F := \\hat x + \\Sigma G' (G \\Sigma G' + R)^{-1}(y - G \\hat x)\n",
    "\\quad \\text{and} \\quad\n",
    "\\Sigma^F := \\Sigma - \\Sigma G' (G \\Sigma G' + R)^{-1} G \\Sigma \\tag{4}\n",
    "$$\n",
    "\n",
    "Here  $ \\Sigma G' (G \\Sigma G' + R)^{-1} $ is the matrix of population regression coefficients of the hidden object $ x - \\hat x $ on the surprise $ y - G \\hat x $.\n",
    "\n",
    "This new density $ p(x \\,|\\, y) = N(\\hat x^F, \\Sigma^F) $ is shown in the next figure via contour lines and the color map.\n",
    "\n",
    "The original density is left in as contour lines for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.grid()\n",
    "\n",
    "Z = gen_gaussian_plot_vals(x_hat, Σ)\n",
    "cs1 = ax.contour(X, Y, Z, 6, colors=\"black\")\n",
    "ax.clabel(cs1, inline=1, fontsize=10)\n",
    "M = Σ * G.T * linalg.inv(G * Σ * G.T + R)\n",
    "x_hat_F = x_hat + M * (y - G * x_hat)\n",
    "Σ_F = Σ - M * G * Σ\n",
    "new_Z = gen_gaussian_plot_vals(x_hat_F, Σ_F)\n",
    "cs2 = ax.contour(X, Y, new_Z, 6, colors=\"black\")\n",
    "ax.clabel(cs2, inline=1, fontsize=10)\n",
    "ax.contourf(X, Y, new_Z, 6, alpha=0.6, cmap=cm.jet)\n",
    "ax.text(float(y[0]), float(y[1]), \"$y$\", fontsize=20, color=\"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Our new density twists the prior $ p(x) $ in a direction determined by  the new\n",
    "information $ y - G \\hat x $.\n",
    "\n",
    "In generating the figure, we set $ G $ to the identity matrix and $ R = 0.5 \\Sigma $ for $ \\Sigma $ defined in [(18.2)](#equation-kalman-dhxs).\n",
    "\n",
    "\n",
    "<a id='kl-forecase-step'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Forecast Step\n",
    "\n",
    "What have we achieved so far?\n",
    "\n",
    "We have obtained probabilities for the current location of the state (missile) given prior and current information.\n",
    "\n",
    "This is called “filtering” rather than forecasting because we are filtering\n",
    "out noise rather than looking into the future.\n",
    "\n",
    "- $ p(x \\,|\\, y) = N(\\hat x^F, \\Sigma^F) $ is called the *filtering distribution*  \n",
    "\n",
    "\n",
    "But now let’s suppose that we are given another task: to predict the location of the missile after one unit of time (whatever that may be) has elapsed.\n",
    "\n",
    "To do this we need a model of how the state evolves.\n",
    "\n",
    "Let’s suppose that we have one, and that it’s linear and Gaussian. In particular,\n",
    "\n",
    "\n",
    "<a id='equation-kl-xdynam'></a>\n",
    "$$\n",
    "x_{t+1} = A x_t + w_{t+1}, \\quad \\text{where} \\quad w_t \\sim N(0, Q) \\tag{5}\n",
    "$$\n",
    "\n",
    "Our aim is to combine this law of motion and our current distribution $ p(x \\,|\\, y) = N(\\hat x^F, \\Sigma^F) $ to come up with a new *predictive* distribution for the location in one unit of time.\n",
    "\n",
    "In view of [(18.5)](#equation-kl-xdynam), all we have to do is introduce a random vector $ x^F \\sim N(\\hat x^F, \\Sigma^F) $ and work out the distribution of $ A x^F + w $ where $ w $ is independent of $ x^F $ and has distribution $ N(0, Q) $.\n",
    "\n",
    "Since linear combinations of Gaussians are Gaussian, $ A x^F + w $ is Gaussian.\n",
    "\n",
    "Elementary calculations and the expressions in [(18.4)](#equation-kl-filter-exp) tell us that\n",
    "\n",
    "$$\n",
    "\\mathbb{E} [A x^F + w]\n",
    "= A \\mathbb{E} x^F + \\mathbb{E} w\n",
    "= A \\hat x^F\n",
    "= A \\hat x + A \\Sigma G' (G \\Sigma G' + R)^{-1}(y - G \\hat x)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\operatorname{Var} [A x^F + w]\n",
    "= A \\operatorname{Var}[x^F] A' + Q\n",
    "= A \\Sigma^F A' + Q\n",
    "= A \\Sigma A' - A \\Sigma G' (G \\Sigma G' + R)^{-1} G \\Sigma A' + Q\n",
    "$$\n",
    "\n",
    "The matrix $ A \\Sigma G' (G \\Sigma G' + R)^{-1} $ is often written as\n",
    "$ K_{\\Sigma} $ and called the *Kalman gain*.\n",
    "\n",
    "- The subscript $ \\Sigma $ has been added to remind us that  $ K_{\\Sigma} $ depends on $ \\Sigma $, but not $ y $ or $ \\hat x $.  \n",
    "\n",
    "\n",
    "Using this notation, we can summarize our results as follows.\n",
    "\n",
    "Our updated prediction is the density $ N(\\hat x_{new}, \\Sigma_{new}) $ where\n",
    "\n",
    "\n",
    "<a id='equation-kl-mlom0'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat x_{new} &:= A \\hat x + K_{\\Sigma} (y - G \\hat x) \\\\\n",
    "    \\Sigma_{new} &:= A \\Sigma A' - K_{\\Sigma} G \\Sigma A' + Q \\nonumber\n",
    "\\end{aligned} \\tag{6}\n",
    "$$\n",
    "\n",
    "- The density $ p_{new}(x) = N(\\hat x_{new}, \\Sigma_{new}) $ is called the *predictive distribution*  \n",
    "\n",
    "\n",
    "The predictive distribution is the new density shown in the following figure, where\n",
    "the update has used parameters.\n",
    "\n",
    "$$\n",
    "A\n",
    "= \\left(\n",
    "\\begin{array}{cc}\n",
    "    1.2 & 0.0 \\\\\n",
    "    0.0 & -0.2\n",
    "\\end{array}\n",
    "  \\right),\n",
    "  \\qquad\n",
    "Q = 0.3 * \\Sigma\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.grid()\n",
    "\n",
    "# Density 1\n",
    "Z = gen_gaussian_plot_vals(x_hat, Σ)\n",
    "cs1 = ax.contour(X, Y, Z, 6, colors=\"black\")\n",
    "ax.clabel(cs1, inline=1, fontsize=10)\n",
    "\n",
    "# Density 2\n",
    "M = Σ * G.T * linalg.inv(G * Σ * G.T + R)\n",
    "x_hat_F = x_hat + M * (y - G * x_hat)\n",
    "Σ_F = Σ - M * G * Σ\n",
    "Z_F = gen_gaussian_plot_vals(x_hat_F, Σ_F)\n",
    "cs2 = ax.contour(X, Y, Z_F, 6, colors=\"black\")\n",
    "ax.clabel(cs2, inline=1, fontsize=10)\n",
    "\n",
    "# Density 3\n",
    "new_x_hat = A * x_hat_F\n",
    "new_Σ = A * Σ_F * A.T + Q\n",
    "new_Z = gen_gaussian_plot_vals(new_x_hat, new_Σ)\n",
    "cs3 = ax.contour(X, Y, new_Z, 6, colors=\"black\")\n",
    "ax.clabel(cs3, inline=1, fontsize=10)\n",
    "ax.contourf(X, Y, new_Z, 6, alpha=0.6, cmap=cm.jet)\n",
    "ax.text(float(y[0]), float(y[1]), \"$y$\", fontsize=20, color=\"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Recursive Procedure\n",
    "\n",
    "\n",
    "<a id='index-1'></a>\n",
    "Let’s look back at what we’ve done.\n",
    "\n",
    "We started the current period with a prior $ p(x) $ for the location $ x $ of the missile.\n",
    "\n",
    "We then used the current measurement $ y $ to update to $ p(x \\,|\\, y) $.\n",
    "\n",
    "Finally, we used the law of motion [(18.5)](#equation-kl-xdynam) for $ \\{x_t\\} $ to update to $ p_{new}(x) $.\n",
    "\n",
    "If we now step into the next period, we are ready to go round again, taking $ p_{new}(x) $\n",
    "as the current prior.\n",
    "\n",
    "Swapping notation $ p_t(x) $ for $ p(x) $ and $ p_{t+1}(x) $ for $ p_{new}(x) $, the full recursive procedure is:\n",
    "\n",
    "1. Start the current period with prior $ p_t(x) = N(\\hat x_t, \\Sigma_t) $.  \n",
    "1. Observe current measurement $ y_t $.  \n",
    "1. Compute the filtering distribution $ p_t(x \\,|\\, y) = N(\\hat x_t^F, \\Sigma_t^F) $ from $ p_t(x) $ and $ y_t $, applying Bayes rule and the conditional distribution [(18.3)](#equation-kl-measurement-model).  \n",
    "1. Compute the predictive distribution $ p_{t+1}(x) = N(\\hat x_{t+1}, \\Sigma_{t+1}) $ from the filtering distribution and [(18.5)](#equation-kl-xdynam).  \n",
    "1. Increment $ t $ by one and go to step 1.  \n",
    "\n",
    "\n",
    "Repeating [(18.6)](#equation-kl-mlom0), the dynamics for $ \\hat x_t $ and $ \\Sigma_t $ are as follows\n",
    "\n",
    "\n",
    "<a id='equation-kalman-lom'></a>\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\hat x_{t+1} &= A \\hat x_t + K_{\\Sigma_t} (y_t - G \\hat x_t) \\\\\n",
    "    \\Sigma_{t+1} &= A \\Sigma_t A' - K_{\\Sigma_t} G \\Sigma_t A' + Q \\nonumber\n",
    "\\end{aligned} \\tag{7}\n",
    "$$\n",
    "\n",
    "These are the standard dynamic equations for the Kalman filter (see, for example, [[LS18](https://python.quantecon.org/zreferences.html#id143)], page 58).\n",
    "\n",
    "\n",
    "<a id='kalman-convergence'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Convergence\n",
    "\n",
    "The matrix $ \\Sigma_t $ is a measure of the uncertainty of our prediction $ \\hat x_t $ of $ x_t $.\n",
    "\n",
    "Apart from special cases, this uncertainty will never be fully resolved, regardless of how much time elapses.\n",
    "\n",
    "One reason is that our prediction $ \\hat x_t $ is made based on information available at $ t-1 $, not $ t $.\n",
    "\n",
    "Even if we know the precise value of $ x_{t-1} $ (which we don’t), the transition equation [(18.5)](#equation-kl-xdynam) implies that $ x_t = A x_{t-1} + w_t $.\n",
    "\n",
    "Since the shock $ w_t $ is not observable at $ t-1 $, any time $ t-1 $ prediction of $ x_t $ will incur some error (unless $ w_t $ is degenerate).\n",
    "\n",
    "However, it is certainly possible that $ \\Sigma_t $ converges to a constant matrix as $ t \\to \\infty $.\n",
    "\n",
    "To study this topic, let’s expand the second equation in [(18.7)](#equation-kalman-lom):\n",
    "\n",
    "\n",
    "<a id='equation-kalman-sdy'></a>\n",
    "$$\n",
    "\\Sigma_{t+1} = A \\Sigma_t A' -  A \\Sigma_t G' (G \\Sigma_t G' + R)^{-1} G \\Sigma_t A' + Q \\tag{8}\n",
    "$$\n",
    "\n",
    "This is a nonlinear difference equation in $ \\Sigma_t $.\n",
    "\n",
    "A fixed point of [(18.8)](#equation-kalman-sdy) is a constant matrix $ \\Sigma $ such that\n",
    "\n",
    "\n",
    "<a id='equation-kalman-dare'></a>\n",
    "$$\n",
    "\\Sigma = A \\Sigma A' -  A \\Sigma G' (G \\Sigma G' + R)^{-1} G \\Sigma A' + Q \\tag{9}\n",
    "$$\n",
    "\n",
    "Equation [(18.8)](#equation-kalman-sdy) is known as a discrete-time Riccati difference equation.\n",
    "\n",
    "Equation [(18.9)](#equation-kalman-dare) is known as a [discrete-time algebraic Riccati equation](https://en.wikipedia.org/wiki/Algebraic_Riccati_equation).\n",
    "\n",
    "Conditions under which a fixed point exists and the sequence $ \\{\\Sigma_t\\} $ converges to it are discussed in [[AHMS96](https://python.quantecon.org/zreferences.html#id105)] and [[AM05](https://python.quantecon.org/zreferences.html#id103)], chapter 4.\n",
    "\n",
    "A sufficient (but not necessary) condition is that all the eigenvalues $ \\lambda_i $ of $ A $ satisfy $ |\\lambda_i| < 1 $ (cf. e.g., [[AM05](https://python.quantecon.org/zreferences.html#id103)], p. 77).\n",
    "\n",
    "(This strong condition assures that the unconditional  distribution of $ x_t $  converges as $ t \\rightarrow + \\infty $.)\n",
    "\n",
    "In this case, for any initial choice of $ \\Sigma_0 $ that is both non-negative and symmetric, the sequence $ \\{\\Sigma_t\\} $ in [(18.8)](#equation-kalman-sdy) converges to a non-negative symmetric matrix $ \\Sigma $ that solves [(18.9)](#equation-kalman-dare)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Implementation\n",
    "\n",
    "\n",
    "<a id='index-2'></a>\n",
    "The class `Kalman` from the [QuantEcon.py](http://quantecon.org/quantecon-py) package implements the Kalman filter\n",
    "\n",
    "- Instance data consists of:  \n",
    "  - the moments $ (\\hat x_t, \\Sigma_t) $ of the current prior.  \n",
    "  - An instance of the [LinearStateSpace](https://github.com/QuantEcon/QuantEcon.py/blob/master/quantecon/lss.py) class from [QuantEcon.py](http://quantecon.org/quantecon-py).  \n",
    "\n",
    "\n",
    "The latter represents a linear state space model of the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    x_{t+1} & = A x_t + C w_{t+1}\n",
    "    \\\\\n",
    "    y_t & = G x_t + H v_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the shocks $ w_t $ and $ v_t $ are IID standard normals.\n",
    "\n",
    "To connect this with the notation of this lecture we set\n",
    "\n",
    "$$\n",
    "Q := CC' \\quad \\text{and} \\quad R := HH'\n",
    "$$\n",
    "\n",
    "- The class `Kalman` from the [QuantEcon.py](http://quantecon.org/quantecon-py) package has a number of methods, some that we will wait to use until we study more advanced applications in subsequent lectures.  \n",
    "- Methods pertinent for this lecture  are:  \n",
    "  - `prior_to_filtered`, which updates $ (\\hat x_t, \\Sigma_t) $ to $ (\\hat x_t^F, \\Sigma_t^F) $  \n",
    "  - `filtered_to_forecast`, which updates the filtering distribution to the predictive distribution – which becomes the new prior $ (\\hat x_{t+1}, \\Sigma_{t+1}) $  \n",
    "  - `update`, which combines the last two methods  \n",
    "  - a `stationary_values`, which computes the solution to [(18.9)](#equation-kalman-dare) and the corresponding (stationary) Kalman gain  \n",
    "\n",
    "\n",
    "You can view the program [on GitHub](https://github.com/QuantEcon/QuantEcon.py/blob/master/quantecon/kalman.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=f1 href=#f1-link><strong>[1]</strong></a> See, for example, page 93 of [Bis06](https://python.quantecon.org/zreferences.html#id108). To get from his expressions to the ones used above, you will also need to apply the [Woodbury matrix identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1619590832.586995,
  "filename": "kalman.md",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "title": "A First Look at the Kalman Filter"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
