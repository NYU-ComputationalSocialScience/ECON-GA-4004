{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Dynamic factors and coincident indices\n",
    "\n",
    "Factor models generally try to find a small number of unobserved \"factors\" that influence a substantial portion of the variation in a larger number of observed variables, and they are related to dimension-reduction techniques such as principal components analysis. Dynamic factor models explicitly model the transition dynamics of the unobserved factors, and so are often applied to time-series data.\n",
    "\n",
    "Macroeconomic coincident indices are designed to capture the common component of the \"business cycle\"; such a component is assumed to simultaneously affect many macroeconomic variables. Although the estimation and use of coincident indices (for example the [Index of Coincident Economic Indicators](http://www.newyorkfed.org/research/regional_economy/coincident_summary.html)) pre-dates dynamic factor models, in several influential papers Stock and Watson (1989, 1991) used a dynamic factor model to provide a theoretical foundation for them.\n",
    "\n",
    "Below, we follow the treatment found in Kim and Nelson (1999), of the Stock and Watson (1991) model, to formulate a dynamic factor model, estimate its parameters via maximum likelihood, and create a coincident index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Macroeconomic data\n",
    "\n",
    "The coincident index is created by considering the comovements in four macroeconomic variables (versions of these variables are available on [FRED](https://research.stlouisfed.org/fred2/); the ID of the series used below is given in parentheses):\n",
    "\n",
    "- Industrial production (IPMAN)\n",
    "- Real aggregate income (excluding transfer payments) (W875RX1)\n",
    "- Manufacturing and trade sales (CMRMTSPL)\n",
    "- Employees on non-farm payrolls (PAYEMS)\n",
    "\n",
    "In all cases, the data is at the monthly frequency and has been seasonally adjusted; the time-frame considered is 1972 - 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "# Get the datasets from FRED\n",
    "start = '1979-01-01'\n",
    "end = '2019-12-01'\n",
    "indprod = DataReader('IPMAN', 'fred', start=start, end=end)\n",
    "income = DataReader('W875RX1', 'fred', start=start, end=end)\n",
    "sales = DataReader('CMRMTSPL', 'fred', start=start, end=end)\n",
    "emp = DataReader('PAYEMS', 'fred', start=start, end=end)\n",
    "# dta = pd.concat((indprod, income, sales, emp), axis=1)\n",
    "# dta.columns = ['indprod', 'income', 'sales', 'emp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = pd.concat((indprod, income, sales, emp), axis=1)\n",
    "dta.columns = ['indprod', 'income', 'sales', 'emp']\n",
    "dta.index.freq = dta.index.inferred_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta.loc[:, 'indprod':'emp'].plot(subplots=True, layout=(2, 2), figsize=(15, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock and Watson (1991) report that for their datasets, they could not reject the null hypothesis of a unit root in each series (so the series are integrated), but they did not find strong evidence that the series were co-integrated.\n",
    "\n",
    "As a result, they suggest estimating the model using the first differences (of the logs) of the variables, demeaned and standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log-differenced series\n",
    "dta['dln_indprod'] = (np.log(dta.indprod)).diff() * 100\n",
    "dta['dln_income'] = (np.log(dta.income)).diff() * 100\n",
    "dta['dln_sales'] = (np.log(dta.sales)).diff() * 100\n",
    "dta['dln_emp'] = (np.log(dta.emp)).diff() * 100\n",
    "\n",
    "# De-mean and standardize\n",
    "dta['std_indprod'] = (dta['dln_indprod'] - dta['dln_indprod'].mean()) / dta['dln_indprod'].std()\n",
    "dta['std_income'] = (dta['dln_income'] - dta['dln_income'].mean()) / dta['dln_income'].std()\n",
    "dta['std_sales'] = (dta['dln_sales'] - dta['dln_sales'].mean()) / dta['dln_sales'].std()\n",
    "dta['std_emp'] = (dta['dln_emp'] - dta['dln_emp'].mean()) / dta['dln_emp'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic factors\n",
    "\n",
    "A general dynamic factor model is written as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_t & = \\Lambda f_t + B x_t + u_t \\\\\n",
    "f_t & = A_1 f_{t-1} + \\dots + A_p f_{t-p} + \\eta_t \\qquad \\eta_t \\sim N(0, I)\\\\\n",
    "u_t & = C_1 u_{t-1} + \\dots + C_q u_{t-q} + \\varepsilon_t \\qquad \\varepsilon_t \\sim N(0, \\Sigma)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $y_t$ are observed data, $f_t$ are the unobserved factors (evolving as a vector autoregression), $x_t$ are (optional) exogenous variables, and $u_t$ is the error, or \"idiosyncratic\", process ($u_t$ is also optionally allowed to be autocorrelated). The $\\Lambda$ matrix is often referred to as the matrix of \"factor loadings\". The variance of the factor error term is set to the identity matrix to ensure identification of the unobserved factors.\n",
    "\n",
    "This model can be cast into state space form, and the unobserved factor estimated via the Kalman filter. The likelihood can be evaluated as a byproduct of the filtering recursions, and maximum likelihood estimation used to estimate the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specification\n",
    "\n",
    "The specific dynamic factor model in this application has 1 unobserved factor which is assumed to follow an AR(2) process. The innovations $\\varepsilon_t$ are assumed to be independent (so that $\\Sigma$ is a diagonal matrix) and the error term associated with each equation, $u_{i,t}$ is assumed to follow an independent AR(2) process.\n",
    "\n",
    "Thus the specification considered here is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{i,t} & = \\lambda_i f_t + u_{i,t} \\\\\n",
    "u_{i,t} & = c_{i,1} u_{1,t-1} + c_{i,2} u_{i,t-2} + \\varepsilon_{i,t} \\qquad & \\varepsilon_{i,t} \\sim N(0, \\sigma_i^2) \\\\\n",
    "f_t & = a_1 f_{t-1} + a_2 f_{t-2} + \\eta_t \\qquad & \\eta_t \\sim N(0, I)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $i$ is one of: `[indprod, income, sales, emp ]`.\n",
    "\n",
    "This model can be formulated using the `DynamicFactor` model built-in to statsmodels. In particular, we have the following specification:\n",
    "\n",
    "- `k_factors = 1` - (there is 1 unobserved factor)\n",
    "- `factor_order = 2` - (it follows an AR(2) process)\n",
    "- `error_var = False` - (the errors evolve as independent AR processes rather than jointly as a VAR - note that this is the default option, so it is not specified below)\n",
    "- `error_order = 2` - (the errors are autocorrelated of order 2: i.e. AR(2) processes)\n",
    "- `error_cov_type = 'diagonal'` - (the innovations are uncorrelated; this is again the default)\n",
    "\n",
    "Once the model is created, the parameters can be estimated via maximum likelihood; this is done using the `fit()` method.\n",
    "\n",
    "**Note**: recall that we have demeaned and standardized the data; this will be important in interpreting the results that follow.\n",
    "\n",
    "**Aside**: in their empirical example, Kim and Nelson (1999) actually consider a slightly different model in which the employment variable is allowed to also depend on lagged values of the factor - this model does not fit into the built-in `DynamicFactor` class, but can be accommodated by using a subclass to implement the required new parameters and restrictions - see Appendix A, below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter estimation\n",
    "\n",
    "Multivariate models can have a relatively large number of parameters, and it may be difficult to escape from local minima to find the maximized likelihood. In an attempt to mitigate this problem, I perform an initial maximization step (from the model-defined starting parameters) using the modified Powell method available in Scipy (see the minimize documentation for more information). The resulting parameters are then used as starting parameters in the standard LBFGS optimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the endogenous data\n",
    "endog = dta.loc['1979-02-01':, 'std_indprod':'std_emp']\n",
    "\n",
    "# Create the model\n",
    "mod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=2, error_order=2)\n",
    "\n",
    "initial_res = mod.fit(method='powell', disp=False)\n",
    "res = mod.fit(initial_res.params, disp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates\n",
    "\n",
    "Once the model has been estimated, there are two components that we can use for analysis or inference:\n",
    "\n",
    "- The estimated parameters\n",
    "- The estimated factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "The estimated parameters can be helpful in understanding the implications of the model, although in models with a larger number of observed variables and / or unobserved factors they can be difficult to interpret.\n",
    "\n",
    "One reason for this difficulty is due to identification issues between the factor loadings and the unobserved factors. One easy-to-see identification issue is the sign of the loadings and the factors: an equivalent model to the one displayed below would result from reversing the signs of all factor loadings and the unobserved factor.\n",
    "\n",
    "Here, one of the easy-to-interpret implications in this model is the persistence of the unobserved factor: we find that exhibits substantial persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary(separate_params=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated factors\n",
    "\n",
    "While it can be useful to plot the unobserved factors, it is less useful here than one might think for two reasons:\n",
    "\n",
    "1. The sign-related identification issue described above.\n",
    "2. Since the data was differenced, the estimated factor explains the variation in the differenced data, not the original data.\n",
    "\n",
    "It is for these reasons that the coincident index is created (see below).\n",
    "\n",
    "With these reservations, the unobserved factor is plotted below, along with the NBER indicators for US recessions. It appears that the factor is successful at picking up some degree of business cycle activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,3))\n",
    "\n",
    "# Plot the factor\n",
    "dates = endog.index._mpl_repr()\n",
    "ax.plot(dates, res.factors.filtered[0], label='Factor')\n",
    "ax.legend()\n",
    "\n",
    "# Retrieve and also plot the NBER recession indicators\n",
    "rec = DataReader('USREC', 'fred', start=start, end=end)\n",
    "ylim = ax.get_ylim()\n",
    "ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-estimation\n",
    "\n",
    "Although here we will be able to interpret the results of the model by constructing the coincident index, there is a useful and generic approach for getting a sense for what is being captured by the estimated factor. By taking the estimated factors as given, regressing them (and a constant) each (one at a time) on each of the observed variables, and recording the coefficients of determination ($R^2$ values), we can get a sense of the variables for which each factor explains a substantial portion of the variance and the variables for which it does not.\n",
    "\n",
    "In models with more variables and more factors, this can sometimes lend interpretation to the factors (for example sometimes one factor will load primarily on real variables and another on nominal variables).\n",
    "\n",
    "In this model, with only four endogenous variables and one factor, it is easy to digest a simple table of the $R^2$ values, but in larger models it is not. For this reason, a bar plot is often employed; from the plot we can easily see that the factor explains most of the variation in industrial production index and a large portion of the variation in sales and employment, it is less helpful in explaining income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.plot_coefficients_of_determination(figsize=(8,2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coincident Index\n",
    "\n",
    "As described above, the goal of this model was to create an interpretable series which could be used to understand the current status of the macroeconomy. This is what the coincident index is designed to do. It is constructed below. For readers interested in an explanation of the construction, see Kim and Nelson (1999) or Stock and Watson (1991).\n",
    "\n",
    "In essence, what is done is to reconstruct the mean of the (differenced) factor. We will compare it to the coincident index on published by the Federal Reserve Bank of Philadelphia (USPHCI on FRED)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usphci = DataReader('USPHCI', 'fred', start=start, end=end)['USPHCI']\n",
    "usphci.plot(figsize=(13,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dusphci = usphci.diff()[1:].values\n",
    "def compute_coincident_index(mod, res):\n",
    "    # Estimate W(1)\n",
    "    spec = res.specification\n",
    "    design = mod.ssm['design']\n",
    "    transition = mod.ssm['transition']\n",
    "    ss_kalman_gain = res.filter_results.kalman_gain[:,:,-1]\n",
    "    k_states = ss_kalman_gain.shape[0]\n",
    "\n",
    "    W1 = np.linalg.inv(np.eye(k_states) - np.dot(\n",
    "        np.eye(k_states) - np.dot(ss_kalman_gain, design),\n",
    "        transition\n",
    "    )).dot(ss_kalman_gain)[0]\n",
    "\n",
    "    # Compute the factor mean vector\n",
    "    factor_mean = np.dot(W1, dta.loc['1972-02-01':, 'dln_indprod':'dln_emp'].mean())\n",
    "    \n",
    "    # Normalize the factors\n",
    "    factor = res.factors.filtered[0]\n",
    "    factor *= np.std(usphci.diff()[1:]) / np.std(factor)\n",
    "\n",
    "    # Compute the coincident index\n",
    "    coincident_index = np.zeros(mod.nobs+1)\n",
    "    # The initial value is arbitrary; here it is set to\n",
    "    # facilitate comparison\n",
    "    coincident_index[0] = usphci.iloc[0] * factor_mean / dusphci.mean()\n",
    "    for t in range(0, mod.nobs):\n",
    "        coincident_index[t+1] = coincident_index[t] + factor[t] + factor_mean\n",
    "    \n",
    "    # Attach dates\n",
    "    coincident_index = pd.Series(coincident_index, index=dta.index).iloc[1:]\n",
    "    \n",
    "    # Normalize to use the same base year as USPHCI\n",
    "    coincident_index *= (usphci.loc['1992-07-01'] / coincident_index.loc['1992-07-01'])\n",
    "\n",
    "    return coincident_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot the calculated coincident index along with the US recessions and the comparison coincident index USPHCI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,3))\n",
    "\n",
    "# Compute the index\n",
    "coincident_index = compute_coincident_index(mod, res)\n",
    "\n",
    "# Plot the factor\n",
    "dates = endog.index._mpl_repr()\n",
    "ax.plot(dates, coincident_index, label='Coincident index')\n",
    "ax.plot(usphci.index._mpl_repr(), usphci, label='USPHCI')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Retrieve and also plot the NBER recession indicators\n",
    "ylim = ax.get_ylim()\n",
    "ax.fill_between(dates[:-3], ylim[0], ylim[1], rec.values[:-4,0], facecolor='k', alpha=0.1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
