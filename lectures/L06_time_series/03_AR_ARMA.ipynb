{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621143a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common Time Series Models: AR, MA, ARMA, ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e55c6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24478a16",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Baseline observations**\n",
    "\n",
    "We create some time series observations from processes that we already know the parameters to.\n",
    "\n",
    "We will use these as a baseline throughout the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135cd029",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "T = 100\n",
    "\n",
    "rho, sigma = 0.7, 0.1\n",
    "\n",
    "# Simulate white noise\n",
    "wn = sigma*np.random.randn(T)\n",
    "\n",
    "# Simulate a random walk\n",
    "rw = np.cumsum(sigma*np.random.randn(T))\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def simulate_ar1(T, rho=0.9, sigma=0.1):\n",
    "    out = np.zeros(T)\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        out[t] = rho*out[t-1] + sigma*np.random.randn()\n",
    "    \n",
    "    return out\n",
    "\n",
    "# Simulate an AR(1)\n",
    "ar = simulate_ar1(T, rho, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaff88e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autocorrelation function and partial autocorrelation function\n",
    "\n",
    "We'll start today by talking about the [autocorrelation function](https://en.wikipedia.org/wiki/Autocorrelation) and [partial autocorrelation function](https://en.wikipedia.org/wiki/Partial_autocorrelation_function).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e979a3d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We defined autocorrelation previously to be\n",
    "\n",
    "$$\\rho_{t, s} = \\text{Corr}(Y_t, Y_s)$$\n",
    "\n",
    "For a stationary process, $\\rho_{t, s} = \\rho_{0, s-t}$ so we will typically refer to the correlation as a \"lagged\" value so\n",
    "\n",
    "$$\\rho_\\tau = \\rho_{0, \\tau} = \\rho_{t, t + \\tau}$$\n",
    "\n",
    "and we'll define the $N$ lag autocorrelation function to be\n",
    "\n",
    "$$\\text{ACF}(N) = \\begin{bmatrix}\\rho_{0} & \\rho_1 & \\dots & \\rho_N \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a1782",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Partial autocorrelation function**\n",
    "\n",
    "The autocorrelation function simply measures the correlation between the random variables that are $\\tau$ apart from one another.\n",
    "\n",
    "Consider the following process:\n",
    "\n",
    "$$y_{t} = \\begin{cases} y_{t-1} \\; \\text{if $t > 0$}\\\\ \\varepsilon_{t} \\; \\text{if $t = 0$} \\end{cases}$$\n",
    "\n",
    "The correlation between any of the lags would be 1, but, in some ways, this doesn't really tell us much... All of the \"interesting\" correlation happens at the first lag.\n",
    "\n",
    "For two elements of the sequence of random variables, $Y_t$ and $Y_s$ ($t<s$), the partial autocorrelation measures the correlation between $Y_t$ and $\\hat{Y}_s$ where $\\hat{Y}_s$ is $Y_s$ with all of the linear explantory power of $\\{Y_{t+1}, Y_{t+2}, \\dots, Y_{s-1}\\}$ removed (something like a \"conditional autocorrelation\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57697a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_acf_pacf(x, nlags=25):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    plot_acf(x, lags=nlags, ax=ax[0])\n",
    "    plot_pacf(x, lags=nlags, ax=ax[1])\n",
    "\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d31e6b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**White noise**\n",
    "\n",
    "White noise shouldn't have any correlation in either the ACF or the PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdb4925",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_acf_pacf(wn, nlags=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a69493",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Autoregressive process**\n",
    "\n",
    "The $\\tau$ lag correlation between two observations would be given by $\\rho^{\\tau}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae63fb55",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Notice now that all correlation is assigned\n",
    "# to the tau=1 lag -- This is because if we\n",
    "# knew Y_{t-1} then we'd have all the relevant\n",
    "# correlations for thinking about Y_t\n",
    "plot_acf_pacf(ar, nlags=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc7b4e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Random walk**\n",
    "\n",
    "The correlation of the random walk would be $\\sqrt{\\frac{1}{s}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a17052",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_acf_pacf(rw, nlags=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db66c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Scatter plots**\n",
    "\n",
    "Scatter plots are great for thinking about time series data... We highly recommend them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fde5c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_ts_scatter(obs):\n",
    "    # t values\n",
    "    x = obs[0:-1]\n",
    "\n",
    "    # t+1 values\n",
    "    y = obs[1:]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    ax.scatter(x, y)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1ba5a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_ts_scatter(wn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe5de3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_ts_scatter(ar);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85d2e6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_ts_scatter(rw);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b76d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving average processes\n",
    "\n",
    "The first time series model that we'll discuss today is the moving average model (MA):\n",
    "\n",
    "$$Y_t = \\sum_{j=0}^\\infty \\phi_j \\varepsilon_{t-j}$$\n",
    "\n",
    "where $\\varepsilon_t \\sim N(0, \\sigma)$ and $\\{\\phi_j\\}$ is square-summable ($\\sum_{j=0}^\\infty \\phi_j^2 < \\infty$)\n",
    "\n",
    "We will often refer to the moving average by the number of non-zero coefficients -- i.e., an MA(q) would be given by:\n",
    "\n",
    "$$Y_t = \\sum_{j=1}^q \\phi_j \\varepsilon_{t-j} + \\sigma \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d96d5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why do we care about this particular process?\n",
    "\n",
    "This particular process is very powerful because of the [Wold representation theorem](https://en.wikipedia.org/wiki/Wold%27s_theorem) which states that,\n",
    "\n",
    "> Every zero-mean stationary process, $\\{Y_t\\}$, can be written as $Y_t = \\eta_t + \\sum_{j=0}^\\infty \\phi_j \\varepsilon_{t-j}$ where $\\eta_t$ is entirely deterministic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351106c2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**White noise?**\n",
    "\n",
    "White noise can be written using an MA process...\n",
    "\n",
    "This one is \"easy\" to guess the right coefficients:\n",
    "\n",
    "$$\\phi_j = \\begin{cases} 1 \\; j=0 \\\\ 0 \\; j \\neq 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290e07bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Autoregressive processes?**\n",
    "\n",
    "We'll think through an AR(1) but similar algebra would allow us to write any autoregressive process as a MA.\n",
    "\n",
    "\\begin{align*}\n",
    "  Y_t &= \\rho Y_{t-1} + \\varepsilon_t \\\\\n",
    "  &= \\rho (\\rho Y_{t-2} + \\varepsilon_{t-1}) + \\varepsilon_t \\\\\n",
    "  &= \\dots \\\\\n",
    "  &= \\sum_{j=0}^\\infty \\rho^j \\varepsilon_{t-j}\n",
    "\\end{align*}\n",
    "\n",
    "thus $\\phi_j = \\rho^j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37406715",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Any process?**\n",
    "\n",
    "Yep. We will skip the proof today, but any process that is weakly stationary (constant mean and autocovariance that only depends on the lag) can be written using an MA process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83265883",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simulating a MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680292e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def simulate_ma(T, phis, sigma):\n",
    "    # Compute the order of MA that we're using\n",
    "    q = len(phis)\n",
    "\n",
    "    # Allocate space\n",
    "    epsilons = sigma*np.random.randn(T+q)\n",
    "    out = np.zeros(T)\n",
    "    for t in range(T):\n",
    "        value = 0.0\n",
    "        for j in range(q):\n",
    "            value += phis[j]*epsilons[q+t-j]\n",
    "        out[t] = value\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454cba0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "phis = np.array([1.0, 0.75, 0.75, 0.5, 0.5, 0.3, 0.3])\n",
    "ma = simulate_ma(1000, phis, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773954bc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15e4ff",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_ts_scatter(ma);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45812799",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Which plot is more helpful for understanding the MA coeffs?\n",
    "plot_acf_pacf(ma);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66df3e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating a MA process\n",
    "\n",
    "The MA model isn't fit using regression -- The $\\varepsilon_t$ values are unobserved and so it is done via a non-linear fitting procedure (maximum likelihood for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337545f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Choosing number of lags**\n",
    "\n",
    "One way to choose a good first guess at the right number of lags is to look at the autocorrelation function.\n",
    "\n",
    "Pick the $q$ such that the autocorrelations at higher lags are zero, i.e. $q$ such that $\\rho_s \\approx 0$  for $s > q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e172383d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Pick q = 6\n",
    "plot_acf_pacf(ma);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d5c1b4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model using statsmodels\n",
    "ma_model = ARIMA(ma, order=(0, 0, 6))\n",
    "res_ma = ma_model.fit()\n",
    "\n",
    "print(res_ma.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907052c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Forecasting a MA process\n",
    "\n",
    "After fitting a MA model, we have access to a variety of methods that we can apply to the fitted model.\n",
    "\n",
    "One of these methods is `get_forecast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e480f0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nforecast = 50\n",
    "\n",
    "forecast_ma = res_ma.get_forecast(nforecast)\n",
    " \n",
    "mf = forecast_ma.predicted_mean\n",
    "ci = forecast_ma.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c42fab",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "tvalues = np.arange(100 + nforecast)\n",
    "\n",
    "ax.plot(tvalues[:100], ma[-100:], color=\"k\")\n",
    "ax.plot(tvalues[100:], mf, color=\"b\", linestyle=\"--\")\n",
    "\n",
    "ax.fill_between(tvalues[100:], ci[:, 0], ci[:, 1], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c9a7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoregressive processes\n",
    "\n",
    "While MA processes have great theoretical properties, sometimes it is convenient to work with AR processes...\n",
    "\n",
    "The generic AR(p) process is written by:\n",
    "\n",
    "$$Y_{t+1} = \\left( \\sum_{j=0}^p \\rho_j Y_{t-j} \\right) + \\sigma \\varepsilon_{t+1}$$\n",
    "\n",
    "Reasons why it can be convenient to work with an AR process rather than an MA:\n",
    "\n",
    "* If the data generating process is an AR(1) then an MA requires an infinite number of parameters while the AR only requires one...\n",
    "* AR models are linear in observed variables which makes regression easier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd76da5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simulating an AR process\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f8c72",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def simulate_ar(T, rhos, sigma):\n",
    "    # Compute the order of AR that we're using\n",
    "    p = len(rhos)\n",
    "\n",
    "    # Allocate space\n",
    "    out = np.zeros(T + 2*p)\n",
    "    for t in range(p, T + 2*p):\n",
    "        value = 0.0\n",
    "        for j in range(p):\n",
    "            value += rhos[j]*out[t-j-1]\n",
    "        out[t] = value + sigma*np.random.randn()\n",
    "\n",
    "    return out[2*p:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f77504",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ar = simulate_ar(1000, np.array([0.7, 0.3, -0.2]), 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e530d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee970ff",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_ts_scatter(ar);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41799c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_acf_pacf(ar);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf9ff7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating an AR process\n",
    "\n",
    "Autoregressive models fit the requirements of a regression, so we can regress $\\{Y_t\\}$ on its lags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18463821",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**How many lags?**\n",
    "\n",
    "One way to choose a good first guess at the right number of lags is to look at the partial autocorrelation function.\n",
    "\n",
    "Pick the $p$ such that the partial autocorrelations at higher lags are zero, i.e. $p$ such that $\\alpha_s \\approx 0$  for $s > p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768e650",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Pick p = 3\n",
    "plot_acf_pacf(ar);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2aacf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model using statsmodels\n",
    "ar_model = ARIMA(ar, order=(3, 0, 0))\n",
    "res_ar = ar_model.fit()\n",
    "\n",
    "print(res_ar.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2dce6c",
   "metadata": {},
   "source": [
    "### Forecasting an AR process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f06407",
   "metadata": {},
   "outputs": [],
   "source": [
    "nforecast = 50\n",
    "\n",
    "forecast_ar = res_ar.get_forecast(nforecast)\n",
    "\n",
    "mf = forecast_ar.predicted_mean\n",
    "ci = forecast_ar.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d644f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "tvalues = np.arange(100 + nforecast)\n",
    "\n",
    "ax.plot(tvalues[:100], ar[-100:], color=\"k\")\n",
    "ax.plot(tvalues[100:], mf, color=\"b\", linestyle=\"--\")\n",
    "\n",
    "ax.fill_between(tvalues[100:], ci[:, 0], ci[:, 1], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff6cc12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ARMA / ARIMA\n",
    "\n",
    "We now present two additions to the models we've seen previously:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0460ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We define an ARMA(p, q) model as the mixture of an AR(p) and MA(q) model\n",
    "\n",
    "$$Y_{t+1} = \\left( \\sum_{j=0}^p \\rho_j Y_{t-j} \\right) + \\left( \\sum_{j=0}^q \\phi_j \\sigma \\varepsilon_{t-j} \\right) + \\sigma \\varepsilon_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa43204",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to discuss the ARIMA model, we need to refresh our memory on the lag operator ($L$).\n",
    "\n",
    "\\begin{align*}\n",
    "  L x_t &= x_{t-1} \\\\\n",
    "  L^2 x_t &= x_{t-2} \\\\\n",
    "  \\dots\n",
    "\\end{align*}\n",
    "\n",
    "then we define an ARIMA(p, d, q) model as the mixture of an AR(p) and MA(q) model with differencing (to help make the process stationary)\n",
    "\n",
    "$$(1 - \\sum_{j=0}^p \\rho_j L^j) (1 - L)^d Y_{t} = \\delta + (1 + \\sum_{j=0}^q \\phi_j L^j) \\sigma \\varepsilon_t$$\n",
    "\n",
    "It's best to see what this results in when we set $d = 1$.\n",
    "\n",
    "$$Y_{t+1} - Y_t = \\sum_{j=0}^p \\rho_j (Y_{t-j} - Y_{t-j-1}) + \\sum_{j=0}^q \\phi_j \\sigma \\varepsilon_{t-j} + \\sigma \\varepsilon_{t+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01ace1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating and forecasting ARIMA process\n",
    "\n",
    "Picking a $p$, $d$, and $q$ is not always obvious...\n",
    "\n",
    "There are [packages](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html) that help with picking the parameters but they basically boil down to \"brute force\" selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab8b19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pandas_datareader import DataReader\n",
    "\n",
    "# Lets load US GDP data\n",
    "gdp = DataReader(\"GDP\", \"fred\", 1975, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079adf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1471dc5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def fit_ARIMA(train, test, p, d, q):\n",
    "    ntrain = train.shape[0]\n",
    "    ntest = test.shape[0]\n",
    "\n",
    "    # Fit model\n",
    "    m = ARIMA(train.to_numpy(), order=(p, d, q))\n",
    "    res = m.fit()\n",
    "    print(res.summary())\n",
    "\n",
    "    forecast = res.get_forecast(ntest)\n",
    "    forecast_mean = forecast.predicted_mean\n",
    "    forecast_ci = forecast.conf_int()\n",
    "\n",
    "    # Make plots\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "    ax[0].plot(train.index, train.to_numpy(), color=\"k\", linewidth=1.5)\n",
    "    ax[0].plot(test.index, test.to_numpy(), color=\"k\", linewidth=1.5, linestyle=\"--\")\n",
    "    ax[0].plot(test.index, forecast_mean, color=\"DarkGreen\", linewidth=1, linestyle=\"--\")\n",
    "    ax[0].fill_between(test.index, forecast_ci[:, 0], forecast_ci[:, 1], color=\"DarkGreen\", alpha=0.45)\n",
    "    ax[0].set_title(\"Forecast\")\n",
    "\n",
    "    ax[1].plot(test.index, test.to_numpy()[:, 0] - forecast_mean)\n",
    "    ax[1].set_title(\"Difference\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b6bc2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train = gdp.loc[:\"2008\"]\n",
    "test = gdp.loc[\"2009\":\"2010\"]\n",
    "\n",
    "\n",
    "fit_ARIMA(train, test, 1, 1, 1);"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
